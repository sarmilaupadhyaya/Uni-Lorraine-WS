{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A very short introduction before to take the plunge\n",
    "\n",
    "<p>The objective fo the course is to understand what is \"Sentiment Analysis and Opinion Mining\" (SAOM).</p>\n",
    "\n",
    "<p>For that, I propose this first session consisting in discovering data used for this NLP task and a first model with all classical steps: data (tokenization, segmentation into train/dev/test parts), programmation of the model, evaluation of the performance.</p>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the positive/negative labels for movie reviews\n",
    "\n",
    "The objective of this programming work is to program a method for predicting the positive/negative label of a movie review.\n",
    "\n",
    "\"Positive\" or \"Negative\" is a binary opinion about something, here, a movie. The review explains what is good and what is not goodin the movie, and then, sommeone (the labeller) labels the review with \"+\" or \"-\" according to a labelling guideline.\n",
    "\n",
    "Thrn, if we have numerous reviews with \"+/-\" labels, it is possible to extract a predictive model which can give the \"+' pr the '-\" opinion to a new review.\n",
    "\n",
    "You will use a movie review database built by Pang and Lee, in 2008. This data provides textual data for movie reviews. I give below two examples of such movie reviews :\n",
    "\n",
    "The former : \"this film is extraordinarily horrendous and I'm not going to waste any more words on it.\" is quite negative.\n",
    "\n",
    "The latter : \"this three hour movie opens up with a view of singer/guitar player/musician/composer frank zappa rehearsing with his fellow band members. All the rest displays a compilation of footage, mostly from the concert at the palladium in new york city, halloween 1979. Other footage shows backstage foolishness, and amazing clay animation by Bruce Bickford. the performance of \"titties and beer\" played in this movie is very entertaining, with drummer terry bozzio supplying the voice of the devil. Frank's guitar solos outdo any van halen or hendrix I've ever heard. Bruce Bickford's outlandish clay animation is that beyond belief with zooms, morphings, etc. and actually, it doesn't even look like clay, it looks like meat.\" gives an positive opinion on the movie.\n",
    "\n",
    "<p>Pang and Lee labeled 1000 movie reviews with the 'positive' label and 1000 movie reviews with the 'negative' label.</p>\n",
    "\n",
    "# Dealing with the movie review database\n",
    "\n",
    "Firstly, let's manipulate the movie review database with Python.\n",
    "\n",
    "This ressource is included in the nltk package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg/cv000_29416.txt\n",
      "['neg/cv000_29416.txt', 'neg/cv001_19502.txt', 'neg/cv002_17424.txt', 'neg/cv003_12683.txt', 'neg/cv004_12641.txt', 'neg/cv005_29357.txt', 'neg/cv006_17022.txt', 'neg/cv007_4992.txt', 'neg/cv008_29326.txt', 'neg/cv009_29417.txt']\n",
      "1000 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading movie_reviews: <urlopen error [WinError\n",
      "[nltk_data]     10060] Une tentative de connexion a échoué car le\n",
      "[nltk_data]     parti connecté n’a pas répondu convenablement au-delà\n",
      "[nltk_data]     d’une certaine durée ou une connexion établie a échoué\n",
      "[nltk_data]     car l’hôte de connexion n’a pas répondu>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"movie_reviews\")\n",
    "\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')\n",
    "\n",
    "print(negids[0])\n",
    "print(negids[0:10])\n",
    "print(len(negids),len(posids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in this example, the package movie_reviews defines a funtion fileids which can list the id of the negative ou positive reviews.\n",
    "\n",
    "For me, the corpus is located in C:\\Users\\langlois\\AppData\\Roaming\\nltk_data\\corpora\\movie_reviews\\neg. I invite you to find such a location, and then, you can open positive and negative review documents. For example, find the cv000_29416.txt negative review.\n",
    "\n",
    "The Python API allows to get the text of a review. You have to give to the function 'word' the list of the ids of the reviews you want the raw content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an', 'accident', '.', 'one', 'of', 'the', 'guys', 'dies', ',', 'but', 'his', 'girlfriend', 'continues', 'to', 'see', 'him', 'in', 'her', 'life', ',', 'and', 'has', 'nightmares', '.', 'what', \"'\", 's', 'the', 'deal', '?', 'watch', 'the', 'movie', 'and', '\"', 'sorta', '\"', 'find', 'out', '.', '.', '.', 'critique', ':', 'a', 'mind', '-', 'fuck', 'movie', 'for', 'the', 'teen', 'generation', 'that', 'touches', 'on', 'a', 'very', 'cool', 'idea', ',', 'but', 'presents', 'it', 'in', 'a', 'very', 'bad', 'package', '.', 'which', 'is', 'what', 'makes', 'this', 'review', 'an', 'even', 'harder', 'one', 'to']\n"
     ]
    }
   ],
   "source": [
    "t = movie_reviews.words(fileids = [negids[0]])\n",
    "print(t[0:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the words are given inside a list, and the tokenization is yet applied. Remark that no lemmatization is applied.\n",
    "\n",
    "OK, now we can access to every movie review, we can access to the document content. Moreover, as the reviews are located in 'pos' and 'neg' directories, we know which ones are 'positive' and which ones are 'negative' (I recall that this labels have been given to reviews by Pang eta al. decisions.\n",
    "\n",
    "Now, we can compute statistics onto this textual material. Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work for you 1\n",
    "\n",
    "Count the frequency of a word into negative documents and positive documents. For each word $w$ in the corpus (all words in all positive and negative documents), you have to give the number of positive documents $POS(w)$ that contain the word, and the number of negative documents $NEG(w)$ that contain $w$.\n",
    "\n",
    "$$POS(w)= |\\{d \\in C_{p}\\ s.t.\\ w \\in d\\}|$$\n",
    "\n",
    "$$NEG(w)= |\\{d \\in C_{n}\\ s.t.\\ w \\in d\\}|$$\n",
    "\n",
    "where $C_{p}$ is the set of positive documents and $C_{n}$ is the set of negative documents.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Compare the values for positive and negative documents. Do you see something interesting?\n",
    "\n",
    "Try it for \"ugly\", \"bad\", \"good\", \"interesting\", \"movie\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ugly neg = 39 pos = 26\n",
      "bad neg = 514 pos = 259\n",
      "good neg = 586 pos = 596\n",
      "interesting neg = 252 pos = 230\n",
      "movie neg = 815 pos = 739\n",
      "the neg = 999 pos = 1000\n"
     ]
    }
   ],
   "source": [
    "## give your code here\n",
    "\n",
    "POS = {}\n",
    "\n",
    "for idDocument in posids:\n",
    "    t = set(movie_reviews.words(fileids = [idDocument]))\n",
    "    for w in t:\n",
    "        if w in POS:\n",
    "            POS[w] += 1\n",
    "        else:\n",
    "            POS[w] = 1\n",
    "            \n",
    "NEG = {}\n",
    "\n",
    "for idDocument in negids:\n",
    "    t = set(movie_reviews.words(fileids = [idDocument]))\n",
    "    for w in t:\n",
    "        if w in NEG:\n",
    "            NEG[w] += 1\n",
    "        else:\n",
    "            NEG[w] = 1\n",
    "\n",
    "\n",
    "for w in ['ugly', 'bad', \"good\", \"interesting\", \"movie\", \"the\"]:\n",
    "    print(w,\"neg =\",NEG[w],\"pos =\",POS[w])\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many different words are in $C_{p} \\cup C_{n}$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb words in pos              : 30417\n",
      "nb words in neg              : 28480\n",
      "nb words in the union        : 39768\n",
      "nb words in the intersection : 19129\n"
     ]
    }
   ],
   "source": [
    "## give your code here\n",
    "\n",
    "words_pos = list(POS.keys())\n",
    "words_neg = list(NEG.keys())\n",
    "words_union = set(words_pos+words_neg)\n",
    "words_intersection = set(words_pos) & set(words_neg)\n",
    "#words_intersection = [w for w in words_pos if w in words_neg]\n",
    "\n",
    "\n",
    "print(\"nb words in pos              :\",len(words_pos))\n",
    "print(\"nb words in neg              :\",len(words_neg))\n",
    "print(\"nb words in the union        :\",len(words_union))\n",
    "print(\"nb words in the intersection :\",len(words_intersection))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the set of words $\\{W_{p/n}\\}$ that are in $C_{p}$ and not in $C_{n}$.\n",
    "\n",
    "In the same way, build the set of words $\\{W_{n/p}\\}$ that are in $C_{n}$ and not in $C_{p}$.\n",
    "\n",
    "Print the most 5 frequent words in $\\{W_{p/n}\\}$ and the most 5 frequent words in $\\{W_{n/p}\\}$.\n",
    "\n",
    "Do you remark if there are clearly 'positive' words in $\\{W_{p/n}\\}$? And if there are clearly 'negative' words in $\\{W_{n/p}\\}$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb of words in POS but not in NEG : 11288\n",
      "nb of words in NEG but not in POS : 9351\n",
      "[('en', 14), ('lovingly', 14), ('melancholy', 13), ('missteps', 12), ('ideals', 12)]\n",
      "[('degenerates', 13), ('horrid', 10), ('pathetically', 10), ('tedium', 10), ('plodding', 9)]\n"
     ]
    }
   ],
   "source": [
    "## give your code here\n",
    "\n",
    "Wpn = [w for w in POS if w not in NEG]\n",
    "Wnp = [w for w in NEG if w not in POS]\n",
    "\n",
    "print(\"nb of words in POS but not in NEG :\",len(Wpn))\n",
    "print(\"nb of words in NEG but not in POS :\",len(Wnp))\n",
    "\n",
    "print(sorted([(w,POS[w]) for w in Wpn],key=lambda x: -x[1])[:5])\n",
    "print(sorted([(w,NEG[w]) for w in Wnp],key=lambda x: -x[1])[:5])\n",
    "\n",
    "\n",
    "## give your comment below\n",
    "## yes 'lovingly' is quite positive, and 'horrid', 'pathetically' are quite negative.But what about 'en'?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work for you 2\n",
    "\n",
    "I have an hypothesis: I am certain that negative reviews tend to be short and that positive reviews tend to be long. Maybe we could use the information 'length of the review' in order to predict the positivity or the negativity of the review?\n",
    "\n",
    "For that, we could firstly compute the average length of the negative reviews and the positive reviews.\n",
    "\n",
    "Extract statistics for the negative reviews and for the positive reviews. We want the histogram of lengths of documents, by buckets of size equal to 100. Do you see a difference of distribution between negative reviews and positive reviews?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## give your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, we want a more precise information.\n",
    "\n",
    "We want the histogram of lengths of documents, by buckets of size equal to 100:\n",
    "\n",
    "+ how many positive documents are of length between 1 and 99?\n",
    "+ how many positive documents are of length between 100 and 199?\n",
    "+ how many positive documents are of length between 200 and 299?\n",
    "+ and so on\n",
    "\n",
    "+ how many negative documents are of length between 1 and 99?\n",
    "+ how many negative documents are of length between 100 and 199?\n",
    "+ how many negative documents are of length between 200 and 299?\n",
    "+ and so on\n",
    "\n",
    "Plot the both histograms on the same graph? Do you see a strong difference of distribution between negative reviews and positive reviews?\n",
    "\n",
    "Then, do you think that the length of the review is a good indicator for the positivity or the negativity of the review?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucket= 9 ([800:899]) : pos = 128 neg =126\n",
      "bucket= 5 ([400:499]) : pos = 85 neg =83\n",
      "bucket= 13 ([1200:1299]) : pos = 31 neg =21\n",
      "bucket= 11 ([1000:1099]) : pos = 74 neg =48\n",
      "bucket= 10 ([900:999]) : pos = 86 neg =98\n",
      "bucket= 8 ([700:799]) : pos = 120 neg =129\n",
      "bucket= 4 ([300:399]) : pos = 56 neg =66\n",
      "bucket= 6 ([500:599]) : pos = 106 neg =139\n",
      "bucket= 14 ([1300:1399]) : pos = 33 neg =12\n",
      "bucket= 7 ([600:699]) : pos = 132 neg =154\n",
      "bucket= 12 ([1100:1199]) : pos = 48 neg =39\n",
      "bucket= 17 ([1600:1699]) : pos = 11 neg =8\n",
      "bucket= 3 ([200:299]) : pos = 23 neg =35\n",
      "bucket= 18 ([1700:1799]) : pos = 9 neg =3\n",
      "bucket= 16 ([1500:1599]) : pos = 14 neg =8\n",
      "bucket= 23 ([2200:2299]) : pos = 2 neg =2\n",
      "bucket= 20 ([1900:1999]) : pos = 4 neg =3\n",
      "bucket= 15 ([1400:1499]) : pos = 17 neg =13\n",
      "bucket= 22 ([2100:2199]) : pos = 5 neg =1\n",
      "bucket= 2 ([100:199]) : pos = 4 neg =6\n",
      "bucket= 19 ([1800:1899]) : pos = 7 neg =4\n",
      "bucket= 29 ([2800:2899]) : pos = 1 neg =0\n",
      "bucket= 21 ([2000:2099]) : pos = 2 neg =0\n",
      "bucket= 28 ([2700:2799]) : pos = 1 neg =0\n",
      "bucket= 27 ([2600:2699]) : pos = 1 neg =0\n",
      "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 27, 28, 29]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## give your code below\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "stats1 = {}\n",
    "for iddoc in posids:\n",
    "    t = movie_reviews.words(fileids = [iddoc])\n",
    "    bucket = len(t) // 100 + 1\n",
    "    if bucket in stats1:\n",
    "        stats1[bucket] += 1\n",
    "    else:\n",
    "        stats1[bucket] = 1\n",
    "\n",
    "stats2 = {}\n",
    "for iddoc in negids:\n",
    "    t = movie_reviews.words(fileids = [iddoc])\n",
    "    bucket = len(t) // 100 + 1\n",
    "    if bucket in stats2:\n",
    "        stats2[bucket] += 1\n",
    "    else:\n",
    "        stats2[bucket] = 1\n",
    "\n",
    "        \n",
    "        \n",
    "for b in stats1:\n",
    "    print(\"bucket=\",b,\"([\"+str((b-1)*100)+\":\"+str((b)*100-1)+\"]) : pos =\",stats1[b],\"neg =\",end=\"\")\n",
    "    if b in stats2:\n",
    "        print(stats2[b])\n",
    "    else:\n",
    "        print(\"0\")\n",
    "        \n",
    "        \n",
    "lb = sorted(list(stats1.keys()))\n",
    "print(lb)\n",
    "\n",
    "pos = [stats1[b] for b in lb]\n",
    "\n",
    "neg = []\n",
    "for b in lb:\n",
    "    if b in stats2:\n",
    "        neg.append(stats2[b])\n",
    "    else:\n",
    "        neg.append(0)\n",
    "    \n",
    "\n",
    "x = np.arange(len(lb))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, pos, width, label='POS')\n",
    "rects2 = ax.bar(x + width/2, neg, width, label='NEG')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## give your comments below\n",
    "## the both distribution are quite similar. length of documents can not be used to predict \n",
    "## the positivity or the negativity of a document\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation of the corpus into a train part and a test part\n",
    "\n",
    "We abandon the idea based on the length.\n",
    "\n",
    "Now we are going to build a predictive model based on the words contained in the review. Our hypothesis is that there are 'positive' words, and 'negative' words. For example, if the review contains 'perfect' it is certainly a positive review.\n",
    "\n",
    "When you want to estimate a predictive model and evaluate it, you have to estimate the predictive model on a train part, and you have to evaluate the predictive performance on a __separate__ test part. Moreover, maybe in the future you will need to estimate hyper-parameters (for example, the weights of the linear combination between two models): then you havealse to deal with a __separate__ development part (also called tuning set).\n",
    "\n",
    "For, that, I propose the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "200\n",
      "200\n",
      "600\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "train_negids = negids[0:int(0.6*len(negids))]\n",
    "dev_negids = negids[int(0.6*len(negids)):int(0.8*len(negids))]\n",
    "test_negids = negids[int(0.8*len(negids)):]\n",
    "\n",
    "train_posids = posids[0:int(0.6*len(posids))]\n",
    "dev_posids = posids[int(0.6*len(posids)):int(0.8*len(posids))]\n",
    "test_posids = posids[int(0.8*len(posids)):]\n",
    "\n",
    "print(len(train_negids))\n",
    "print(len(dev_negids))\n",
    "print(len(test_negids))\n",
    "print(len(train_posids))\n",
    "print(len(dev_posids))\n",
    "print(len(test_posids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The predictive model\n",
    "\n",
    "[Dave et al., 2003] propose the following strategy to predict the label of a movie review :\n",
    "\n",
    "$$\\mathcal{P}$$\n",
    "\n",
    "The score of a word $w$ is defined by:\n",
    "\n",
    "$$ score(w) = \\frac{P(w|\\mathcal{P})-P(w|\\mathcal{N})}{P(w|\\mathcal{P})+P(w|\\mathcal{N})}$$\n",
    "\n",
    "Where $P(w|\\mathcal{P})$ is the probability that the word $w$ occurs in a positive document, and $P(w|\\mathcal{N})$ is the probability that the word $w$ occurs in a negative document of the training corpus.\n",
    "\n",
    "Then, the 'positivity' of a document $d$ is given by:\n",
    "\n",
    "$$eval(d) = \\sum_{w\\ \\in \\ set(d)} score(w)$$\n",
    "\n",
    "where the sum is applyed on the set of words in $d$ (a word occuring several times in $d$ is counted only once in the sum).\n",
    "\n",
    "Then, the decision follows the following condition: if  $eval(d) > 0$ then a document in the test part positive else the document is negative\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating the parameters of the predictive model\n",
    "\n",
    "The parameters of this predictive model are all the $P(w|\\mathcal{P})$ and the $P(w|\\mathcal{N})$ for all the words $w$ in the positive and negative documents.\n",
    "\n",
    "__Work__: compute these values on the training corpus.\n",
    "\n",
    "For estimating the $P(w|\\mathcal{P})$, iterate on all the positive documents in the train positive part. $P(w|\\mathcal{P})$ is estimated by the formula:\n",
    "\n",
    "$$P(w|\\mathcal{P}) = \\frac{|sum_{d \\in \\{\\mathcal{P}_{train}\\}} \\delta(w,d)|}{|\\{\\mathcal{P}_{train}\\}|}$$\n",
    "\n",
    "where $\\{\\mathcal{P}_{train}\\}$ is the set of documents in the positive train part, and  $\\delta(w,d)$ is equal to 1 if $w$ is in $d$, 0 otherwise. \n",
    "\n",
    "The formula is the same for $P(w|\\mathcal{N})$:\n",
    "\n",
    "$$P(w|\\mathcal{N}) = \\frac{|sum_{d \\in \\{\\mathcal{N}_{train}\\}} \\delta(w,d)|}{|\\{\\mathcal{N}_{train}\\}|}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(not,positive)  = 0.8333333333333334\n",
      "P(not,negative)  = 0.8166666666666667\n",
      "P(so,positive)   = 0.7283333333333334\n",
      "P(so,negative)   = 0.7616666666666667\n",
      "P(good,positive) = 0.5683333333333334\n",
      "P(good,negative) = 0.59\n",
      "P(ugly,positive) = 0.023333333333333334\n",
      "P(ugly,negative) = 0.03666666666666667\n"
     ]
    }
   ],
   "source": [
    "## give your code below\n",
    "\n",
    "## for P(w|negative)\n",
    "stats_neg_words = {}\n",
    "\n",
    "for iddoc in train_negids:\n",
    "    t = movie_reviews.words(fileids = [iddoc])\n",
    "    t = set(t) ## now a word occurs only once \n",
    "    for w in t:\n",
    "        if w in stats_neg_words:\n",
    "            stats_neg_words[w] += 1\n",
    "        else:\n",
    "            stats_neg_words[w] = 1\n",
    "\n",
    "## we divide by the number of negative docs in order to have values between 0 and 1            \n",
    "for w in stats_neg_words:\n",
    "    stats_neg_words[w] = stats_neg_words[w]/len(train_negids) \n",
    "\n",
    "## same work for positive documents\n",
    "stats_pos_words = {}\n",
    "\n",
    "for iddoc in train_posids:\n",
    "    t = movie_reviews.words(fileids = [iddoc])\n",
    "    t = set(t)\n",
    "    for w in t:\n",
    "        if w in stats_pos_words:\n",
    "            stats_pos_words[w] += 1\n",
    "        else:\n",
    "            stats_pos_words[w] = 1\n",
    "\n",
    "for w in stats_pos_words:\n",
    "    stats_pos_words[w] = stats_pos_words[w]/len(train_posids) \n",
    "    \n",
    "## we complete stats_pos_words by adding each\n",
    "## word that are in stats_neg_words and not in stats_pos_words\n",
    "## but with score 0\n",
    "for w in stats_neg_words:\n",
    "  if not w in stats_pos_words:\n",
    "    stats_pos_words[w] = 0\n",
    "\n",
    "## same work for stats_neg_words\n",
    "for w in stats_pos_words:\n",
    "  if not w in stats_neg_words:\n",
    "    stats_neg_words[w] = 0\n",
    "\n",
    "    \n",
    "## examples    \n",
    "print(\"P(not,positive)  =\",stats_pos_words[\"not\"])\n",
    "print(\"P(not,negative)  =\",stats_neg_words[\"not\"])\n",
    "print(\"P(so,positive)   =\",stats_pos_words[\"so\"])\n",
    "print(\"P(so,negative)   =\",stats_neg_words[\"so\"])\n",
    "print(\"P(good,positive) =\",stats_pos_words[\"good\"])\n",
    "print(\"P(good,negative) =\",stats_neg_words[\"good\"])\n",
    "\n",
    "print(\"P(ugly,positive) =\",stats_pos_words[\"ugly\"])\n",
    "print(\"P(ugly,negative) =\",stats_neg_words[\"ugly\"])\n",
    "\n",
    "\n",
    "\n",
    "## P(w|?) ? can be positive or negative if stats is positive dictionary or negative dictionary\n",
    "def score_word(w,stats):\n",
    "    if w not in stats:\n",
    "        return 0\n",
    "    return stats[w]\n",
    "\n",
    "## positivity of the word\n",
    "def score(w,stats_p,stats_n):\n",
    "    ## ATTENTION : I must take into account that, during test conditions,\n",
    "    ## w can be not in the training positive docs and not int the training negative docs\n",
    "    sp = score_word(w,stats_p)\n",
    "    sn = score_word(w,stats_n)\n",
    "    if sp+sn == 0:\n",
    "        ## then, by default return 0 : not negative, not positive\n",
    "        return 0\n",
    "    return (sp-sn)/(sp+sn)\n",
    "\n",
    "\n",
    "## positivity of a document\n",
    "def eval(iddoc,stats_p,stats_n,threshold):\n",
    "    t = movie_reviews.words(fileids = [iddoc])\n",
    "    t = set(t)\n",
    "    s = 0\n",
    "    ## sum for all word in the document\n",
    "    for w in t:\n",
    "        s += score(w,stats_p,stats_n)\n",
    "    ## return decision\n",
    "    if s > threshold:\n",
    "        return \"P\"\n",
    "    else:\n",
    "        return \"N\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the predictive model\n",
    "\n",
    "We want to know if, on the test corpus, the decision strategy described above performs well or not. We want to know if when a document is labeled positive by Pang et al., the document is predicted as positive by the model, and so on for a negative document.\n",
    "\n",
    "We are going to evaluate the following statistics on the test corpus.\n",
    "\n",
    "When the model predict 'positive', the document could be actually 'positive': this is a true positive ($tp$). But, the document may be actually 'negative': this is a false positive ($fp$).\n",
    "\n",
    "In the same way, when the model predict 'negative', the document could be actually 'negative': this is a true negative ($tn$). But, the document may be actually 'positive': this is a false negative ($fn$).\n",
    "\n",
    "Note that, there are the following constraints:\n",
    "\n",
    "+ $tp+fp=n$\n",
    "+ $fn+tn=n$\n",
    "+ $tp+fn=n$\n",
    "+ $fp+tn=n$ \n",
    "\n",
    "where $n$ is the number of documents (positive or negative).\n",
    "\n",
    "these notations can be sumarized into the following table\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>  </td>\n",
    "        <td>  </td>\n",
    "        <td colspan=2 style=\"text-align: center\"> True label </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>  </td>\n",
    "        <td>  </td>\n",
    "        <td> positive </td>\n",
    "        <td> negative </td>\n",
    "    </tr>    \n",
    "    <tr>\n",
    "        <td rowspan=2 style=\"vertical-align: center\"> Predicted label </td>\n",
    "        <td> positive </td>\n",
    "        <td> true positive </td>\n",
    "        <td> false positive </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> negative </td>\n",
    "        <td> false negative </td>\n",
    "        <td> true negative </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Be careful: unfortunately, there is a trap in the used vocabulary; you have the labels 'positive' and 'negative' and these labels can be 'true positive', 'true negative', 'false positive', or 'false negative'. This is confusing. Sorry for that.\n",
    "\n",
    "The recall evaluates how much the model can retrieve the correct decision:\n",
    "\n",
    "$$recall = \\frac{tp}{tp+fn}$$\n",
    "\n",
    "The precision evaluates if the model does not sur-generate the positive or negative prediction:\n",
    "\n",
    "$$precision = \\frac{tp}{tp+fp}$$\n",
    "\n",
    "Moreover, we can use the F1 measure which deals with precision and recall:\n",
    "\n",
    "$$F1 = 2 \\times \\frac{precision \\times recall}{precision + recall}$$\n",
    "\n",
    "__Work__: evaluate on the test corpus the recall, the precision and the F1 measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== 0\n",
      "recall 0.935\n",
      "precision 0.677536231884058\n",
      "F1 0.7857142857142856\n"
     ]
    }
   ],
   "source": [
    "## we predict P or N for each doc in test, and we compute tp, fp, tn and fn\n",
    "\n",
    "t = 0\n",
    "\n",
    "print(\"==================\",t)\n",
    "\n",
    "tp = 0\n",
    "fp = 0\n",
    "tn = 0\n",
    "fn = 0\n",
    "\n",
    "for iddoc in test_negids:\n",
    "    if eval(iddoc,stats_pos_words,stats_neg_words,t) == \"P\":\n",
    "        fp += 1\n",
    "    else:\n",
    "        tn += 1\n",
    "\n",
    "for iddoc in test_posids:\n",
    "    if eval(iddoc,stats_pos_words,stats_neg_words,t) == \"P\":\n",
    "        tp += 1\n",
    "    else:\n",
    "        fn += 1\n",
    "\n",
    "recall = tp/(tp+fn)\n",
    "precision = tp/(tp+fp)\n",
    "F1 = 2*(precision*recall)/(precision+recall)\n",
    "\n",
    "print(\"recall\",recall)\n",
    "print(\"precision\",precision)\n",
    "print(\"F1\",F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning the model\n",
    "\n",
    "In the model by \\[Dave et al., 2003\\] a document $d$ is considered as positive if $eval(d)>0$. But maybe this value is not very precise. For exemple, if a document has a value $eval$ equal to 0.02, it has chance to be negative.\n",
    "\n",
    "To check that, replace the decision $eval(d)>0 \\Rightarrow positive$ by  $eval(d)>\\alpha \\Rightarrow positive$ where $\\alpha$ is a threshold varying from a negative value to a positive value. Explore the range values of the $eval$ score on the test corpus. Iterate $\\alpha$ from the minimum value of $\\alpha$ to the maximum value of $\\alpha$. You can not scan all the values: choose a step, and refine. For each tested value, compute the recall and the precision on the __development__ part, and graph the curve (precision,recall).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== 0\n",
      "recall 0.98\n",
      "precision 0.6829268292682927\n",
      "F1 0.8049281314168377\n",
      "================== 1\n",
      "recall 0.975\n",
      "precision 0.7065217391304348\n",
      "F1 0.8193277310924371\n",
      "================== 2\n",
      "recall 0.965\n",
      "precision 0.7394636015325671\n",
      "F1 0.8373101952277658\n",
      "================== 3\n",
      "recall 0.94\n",
      "precision 0.7580645161290323\n",
      "F1 0.8392857142857143\n",
      "================== 4\n",
      "recall 0.89\n",
      "precision 0.7705627705627706\n",
      "F1 0.8259860788863109\n",
      "================== 5\n",
      "recall 0.85\n",
      "precision 0.8095238095238095\n",
      "F1 0.8292682926829269\n",
      "================== 6\n",
      "recall 0.825\n",
      "precision 0.8461538461538461\n",
      "F1 0.8354430379746836\n",
      "================== 7\n",
      "recall 0.785\n",
      "precision 0.8579234972677595\n",
      "F1 0.8198433420365535\n",
      "================== 8\n",
      "recall 0.77\n",
      "precision 0.8603351955307262\n",
      "F1 0.812664907651715\n",
      "================== 9\n",
      "recall 0.75\n",
      "precision 0.8875739644970414\n",
      "F1 0.8130081300813007\n",
      "================== 10\n",
      "recall 0.72\n",
      "precision 0.8888888888888888\n",
      "F1 0.7955801104972375\n"
     ]
    }
   ],
   "source": [
    "## give the code here\n",
    "\n",
    "## we predict P or N for each doc in dev, and we compute tp, fp, tn and fn\n",
    "\n",
    "for t in [0,1,2,3,4,5,6,7,8,9,10]:\n",
    "    print(\"==================\",t)\n",
    "\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "\n",
    "    for iddoc in dev_negids:\n",
    "        if eval(iddoc,stats_pos_words,stats_neg_words,t) == \"P\":\n",
    "            fp += 1\n",
    "        else:\n",
    "            tn += 1\n",
    "\n",
    "    for iddoc in dev_posids:\n",
    "        if eval(iddoc,stats_pos_words,stats_neg_words,t) == \"P\":\n",
    "            tp += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "\n",
    "    recall = tp/(tp+fn)\n",
    "    precision = tp/(tp+fp)\n",
    "    F1 = 2*(precision*recall)/(precision+recall)\n",
    "\n",
    "    print(\"recall\",recall)\n",
    "    print(\"precision\",precision)\n",
    "    print(\"F1\",F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then check for which value of $\\alpha$ the F1 measure is the highest. Apply this value on the test part. Is the F1 measure increased?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== 0.94\n",
      "recall 0.93\n",
      "precision 0.6940298507462687\n",
      "F1 0.7948717948717949\n"
     ]
    }
   ],
   "source": [
    "## give the code below\n",
    "\n",
    "t = 0.94\n",
    "\n",
    "print(\"==================\",t)\n",
    "\n",
    "tp = 0\n",
    "fp = 0\n",
    "tn = 0\n",
    "fn = 0\n",
    "\n",
    "for iddoc in test_negids:\n",
    "    if eval(iddoc,stats_pos_words,stats_neg_words,t) == \"P\":\n",
    "        fp += 1\n",
    "    else:\n",
    "        tn += 1\n",
    "\n",
    "for iddoc in test_posids:\n",
    "    if eval(iddoc,stats_pos_words,stats_neg_words,t) == \"P\":\n",
    "        tp += 1\n",
    "    else:\n",
    "        fn += 1\n",
    "\n",
    "recall = tp/(tp+fn)\n",
    "precision = tp/(tp+fp)\n",
    "F1 = 2*(precision*recall)/(precision+recall)\n",
    "\n",
    "print(\"recall\",recall)\n",
    "print(\"precision\",precision)\n",
    "print(\"F1\",F1)\n",
    "\n",
    "\n",
    "## give your comment below\n",
    "\n",
    "# Tuning the threshold allows to increase the performance, but this is certainly not significant\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
